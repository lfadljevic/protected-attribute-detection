{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9531a6",
   "metadata": {},
   "source": [
    "# Semi-Automated Protected Attributes Detection over Sample/Label Biases\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fairlens as fl\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from scipy.stats import entropy, pearsonr, chi2_contingency, pointbiserialr, f_oneway, variation, kruskal\n",
    "from statsmodels.stats.oneway import effectsize_oneway\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats.contingency import association\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "\n",
    "from packages.fairness_report import FairnessReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f04398",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASETS = {\n",
    "    45068: '43141_adult',\n",
    "    46356: '46356_german_credit',\n",
    "    45069: '45069_diabetes',\n",
    "    43904: '43904_law_bar'   \n",
    "}\n",
    "\n",
    "for k in DATASETS:\n",
    "    dataset = fetch_openml(data_id=k, as_frame=True)\n",
    "    dataset.data.to_csv(DATASETS[k] + '_raw.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d38a2f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Results reproduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    45068,   # adult                         JOB/INCOME\n",
    "    44096,   # GermanCreditData              FINANCE/BANKING\n",
    "    45069,   # Diabetes130US                 HEALTH\n",
    "    43904    # law-school-admission-bianry   EDUCATION\n",
    "]\n",
    "\n",
    "\n",
    "class Dsettings:\n",
    "    def __init__(self, target, privileged, target_description, favorable_label, unfavorable_label):\n",
    "        self.target = target\n",
    "        self.privileged = privileged\n",
    "        self.target_description = target_description\n",
    "        self.favorable_label = favorable_label\n",
    "        self.unfavorable_label = unfavorable_label\n",
    "\n",
    "DSETTINGS = {\n",
    "    0: Dsettings('class', '>50K', 'People earning over 50.000$', 1, 0),\n",
    "    1: Dsettings('class', 1, 'People who were classified as good customers for a loan', 1, 0),\n",
    "    2: Dsettings('class', 'NO', 'People NOT readmissioned to the hospital', 0, 1),\n",
    "    3: Dsettings('ugpagt3', 'TRUE', 'People with admission (ugpa > 3)', 1, 0)\n",
    "}\n",
    "\n",
    "DRANKINGS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71428e9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Repeat for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686307d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 0\n",
    "bins = 6\n",
    "\n",
    "dataset = fetch_openml(data_id=DATASETS[dataset_id], as_frame=True)\n",
    "df = dataset.data\n",
    "    \n",
    "df = df.replace('nan', np.nan)\n",
    "df = df.dropna()    \n",
    "\n",
    "if dataset_id == 2:\n",
    "    dataset.target.loc[dataset.target != 'NO'] = 'YES'\n",
    "        \n",
    "df_copy = df.copy()\n",
    "for c in df_copy.columns:\n",
    "    df_copy[c] = df_copy[c].astype(str)\n",
    "    \n",
    "protected_attributes = list(fl.sensitive.detect_names_df(df_copy, deep_search=True).keys())\n",
    "\n",
    "for pa in protected_attributes:\n",
    "    if pa.lower() == 'age':\n",
    "        try:\n",
    "            df = df[df[pa] <= 100]\n",
    "        except:\n",
    "            # already categorised\n",
    "            continue\n",
    "        \n",
    "    if df[pa].dtype == 'category':\n",
    "        df[pa] = df[pa].astype('object')\n",
    "        \n",
    "    if df[pa].dtype != 'object':        \n",
    "        if len(pd.unique(df[pa])) > 10:\n",
    "            df[pa] = pd.cut(df[pa], bins=bins)\n",
    "        df[pa] = df[pa].astype('object')\n",
    "        \n",
    "for c in df.columns:\n",
    "    if (df[c].dtype == 'category'):\n",
    "        df[c] = df[c].astype('object')\n",
    "    elif (len(pd.unique(df[c])) <= 5):\n",
    "        df[c] = df[c].astype('object') \n",
    "        \n",
    "    if (df[c].dtype == 'int64' or df[c].dtype == 'float64') and (len(pd.unique(df[c])) > 20):\n",
    "        df[c] = pd.cut(df[c], bins=bins)\n",
    "        df[c] = df[c].astype('object')\n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea69532",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "numeric_data = df.select_dtypes(include=['int64', 'float64', 'uint8'])\n",
    "categorical_data = df.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "try:\n",
    "    categorical_data[dataset.target_names[0]] = dataset.target\n",
    "except:\n",
    "    # that means that target attribute is the last column in a dataframe\n",
    "    pass\n",
    "    \n",
    "no_numeric = False\n",
    "try:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_data)\n",
    "except:\n",
    "    # no numerical data\n",
    "    no_numeric = True\n",
    "    pass\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "if no_numeric:\n",
    "    data = categorical_data.apply(encoder.fit_transform)\n",
    "    data = data.astype(object)\n",
    "    categorical_present = True\n",
    "else:\n",
    "    try:\n",
    "        encoded_data = categorical_data.apply(encoder.fit_transform)\n",
    "        encoded_data[encoded_data.columns.tolist()] = encoded_data[encoded_data.columns.tolist()].astype(np.object)\n",
    "        data = pd.concat([pd.DataFrame(scaled_data, columns=numeric_data.columns), encoded_data.reset_index(drop=True)], axis=1)\n",
    "        categorical_present = True\n",
    "    except:\n",
    "        # no categorical attritbutes present in the observed dataset -> data = scaled numeric data\n",
    "        categorical_present = False\n",
    "        data = pd.DataFrame(scaled_data, columns=numeric_data.columns)\n",
    "\n",
    "data = data.drop_duplicates()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if categorical_present:\n",
    "    categorical_data = df.select_dtypes(include=['object', 'category'])\n",
    "    \n",
    "    try:\n",
    "        categorical_data[dataset.target_names[0]] = dataset.target\n",
    "    except:\n",
    "        # last column is the target label/attribute\n",
    "        pass\n",
    "\n",
    "    for c in categorical_data.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(categorical_data[c])\n",
    "        le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "        #print()\n",
    "        #print('Attribute: ' + c)\n",
    "        #print(le_name_mapping)\n",
    "#else:\n",
    "    #print('No categorical attributes found in the dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e434a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### (Hidden) Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b405297",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_matrix = {}\n",
    "correlation_matrix = pd.DataFrame(index=data.columns, columns=data.columns)\n",
    "for col1 in data.columns:\n",
    "    for col2 in data.columns:\n",
    "        if col1 != col2:\n",
    "            if (data[col1].dtype == np.object and data[col2].dtype == np.object):\n",
    "                # Chi2 test + Cramer's V test\n",
    "                contingency_table = pd.crosstab(data[col1], data[col2])\n",
    "                chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "                \n",
    "                if (p < 0.05):\n",
    "                    cramers_v_value = association(contingency_table)\n",
    "                    correlation_matrix.at[col1, col2] = cramers_v_value\n",
    "                    significance_matrix[(col1, col2)] = p\n",
    "            elif ((data[col1].dtype == np.object and data[col2].dtype != np.object) or\n",
    "                  (data[col1].dtype != np.object and data[col2].dtype == np.object)):\n",
    "                if (data[col1].dtype == np.object):\n",
    "                    cat = data[col1]\n",
    "                    numeric = data[col2]\n",
    "                else:\n",
    "                    cat = data[col2]\n",
    "                    numeric = data[col1]\n",
    "                    \n",
    "                # One-Way ANOVA with effect size (multi-class categorical variable) -> omega squared effect size\n",
    "                if (len(pd.unique(cat)) > 2):\n",
    "                    q = data[[numeric.name, cat.name]]\n",
    "                    q[cat.name] = pd.to_numeric(q[cat.name])\n",
    "                    \n",
    "                    model = ols('Q(\\\"' + numeric.name + '\\\")' + ' ~ C(Q(\\\"' + cat.name + '\\\"))', data=q).fit()\n",
    "                    aov = sm.stats.anova_lm(model, typ=2)\n",
    "                    \n",
    "                    if (aov['PR(>F)'].iloc[0] < 0.05):\n",
    "                        aov['mean_sq'] = aov[:]['sum_sq']/aov[:]['df']\n",
    "                        aov['eta_sq'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])\n",
    "                        aov['omega_sq'] = (aov[:-1]['sum_sq']-(aov[:-1]['df']*aov['mean_sq'][-1]))/(sum(aov['sum_sq'])+aov['mean_sq'][-1])\n",
    "                        correlation_matrix.at[col1, col2] = aov['omega_sq'].iloc[0]\n",
    "                        significance_matrix[(col1, col2)] = aov['PR(>F)'].iloc[0]\n",
    "                else:\n",
    "                    # Point-Biserial (binary categorical variable)\n",
    "                    pointbiserial_corr, pointbiserial_p_value = pointbiserialr(cat, numeric)\n",
    "                    if (pointbiserial_p_value < 0.05):\n",
    "                        correlation_matrix.at[col1, col2] = pointbiserial_corr\n",
    "                        significance_matrix[(col1, col2)] = pointbiserial_p_value\n",
    "            else:                \n",
    "                # Pearson's correlation\n",
    "                coef, p_value = pearsonr(data[col1], data[col2])\n",
    "                if (p_value < 0.05):\n",
    "                    correlation_matrix.at[col1, col2] = coef\n",
    "                    significance_matrix[(col1, col2)] = p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_main_diagonal = correlation_matrix.where(np.tril(np.ones(correlation_matrix.shape), k=-1).astype(np.bool))\n",
    "q = below_main_diagonal.fillna(0).stack()\n",
    "\n",
    "top_largest = q.nlargest(15)\n",
    "top_smallest = q.nsmallest(5)\n",
    "\n",
    "candidates = pd.concat([top_largest, top_smallest])\n",
    "\n",
    "candidates = pd.DataFrame(candidates, columns=['corr'])\n",
    "candidates['abs_corr'] = abs(candidates['corr']) \n",
    "candidates = candidates.sort_values(by='abs_corr', ascending=False)\n",
    "candidates = candidates['corr'].head(15)\n",
    "\n",
    "\n",
    "try:\n",
    "    exclude = list(protected_attributes) + [dataset.target_names[0]]\n",
    "except:\n",
    "    exclude = list(protected_attributes) + [data.columns[-1]]\n",
    "\n",
    "attribute_pairs = candidates.index.values.tolist()\n",
    "\n",
    "result = pd.Series()\n",
    "consumed_attributes = []\n",
    "for i, attribute_pair in enumerate(attribute_pairs):\n",
    "    att1 = attribute_pair[0]\n",
    "    att2 = attribute_pair[1]\n",
    "\n",
    "    if (att1 not in protected_attributes and att2 not in protected_attributes):\n",
    "        continue\n",
    "    if (att1 in exclude or att1 in consumed_attributes) and (att2 in exclude or att2 in consumed_attributes):\n",
    "        continue    \n",
    "    if (att1 not in exclude and att1 not in consumed_attributes):\n",
    "        consumed_attributes.append(att1)\n",
    "    if (att2 not in exclude and att2 not in consumed_attributes):\n",
    "        consumed_attributes.append(att2)\n",
    "\n",
    "    result = pd.concat([result, pd.Series(data={candidates.index[i]: candidates[i]})])\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = pd.unique(list(itertools.chain.from_iterable(result.index.values.tolist())))\n",
    "focus_attributes = [a for a in q if a not in exclude]\n",
    "focus_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faeff72",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Metrics report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FairnessReport(dataset, df, data)\n",
    "\n",
    "focus = protected_attributes + focus_attributes\n",
    "stats = fr.detect_protected_attributes(\n",
    "    data[focus], \n",
    "    DSETTINGS[dataset_id].favorable_label, \n",
    "    DSETTINGS[dataset_id].unfavorable_label)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee3909",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Metrics ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24849fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"higher is better\" in the context how sensitive an attribute actually is \n",
    "# ranking score   -> the least sensitive attribute gets the best score (e.g., 1) for observed metric\n",
    "# ranking overall -> the most sensitive attribute will have the highest sum of ranking scores, while\n",
    "#                    the least sensitive the lowest sum of ranking scores\n",
    "higher_is_better = {\n",
    "    'entropy': True,                           # the higher the entropy, the less sensitive an attribute is\n",
    "    'imbalance_ratio': False,                  # the higher the imbalance ratio, the more sensitive an attribute is\n",
    "    'imbalance_degree': False,                 # the higher the imbalance degree, the more sensitive an attribute is\n",
    "    'statistical_parity_difference': False,    # the higher the (absolute) statistical parity difference, the more sensitive an attribute is\n",
    "    'disparate_impact_ratio': False,           # the higher the difference (\"DIR - 1\" observation), the more sensitive an attribute is (difference of 0 would indicate perfect fairness)\n",
    "    'smoothed_edf': True                       # the higher the smoothed EDF, the less sensitive an attribute (value of 1 would indicate perfect fairness)\n",
    "}\n",
    "\n",
    "def calculate_rank(values):\n",
    "    sorted_values = values.sort_values(ascending=not higher_is_better[metric])\n",
    "    ranks = sorted_values.rank(method='dense', ascending=not higher_is_better[metric])\n",
    "    return ranks\n",
    "\n",
    "ranks = stats[stats.columns[3:]]\n",
    "\n",
    "drop = []\n",
    "for metric in ranks.columns:\n",
    "    try:\n",
    "        if metric == 'disparate_impact_ratio':\n",
    "            ranks[metric] = calculate_rank(abs(ranks[metric] - 1))\n",
    "        elif metric == 'imbalance_degree':\n",
    "            ranks[metric] = calculate_rank(ranks[metric] / stats['total_classes'])\n",
    "        elif metric == 'statistical_parity_difference':\n",
    "            ranks[metric] = calculate_rank(abs(ranks[metric]))\n",
    "        else:\n",
    "            ranks[metric] = calculate_rank(ranks[metric])\n",
    "    except:\n",
    "        drop.append(metric)\n",
    "\n",
    "if len(drop) > 0:\n",
    "    ranks = ranks.drop(drop, axis=1)\n",
    "    \n",
    "ranks = ranks.astype(int)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae018ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_metrics = ['entropy', 'imbalance_ratio', 'imbalance_degree']\n",
    "aif360_metrics = ['statistical_parity_difference', 'disparate_impact_ratio', 'smoothed_edf']\n",
    "\n",
    "dist_rankings = pd.DataFrame(ranks[distribution_metrics].sum(axis=1).sort_values(ascending=False), columns=['RS*'])\n",
    "dist_rankings['R*'] = list(range(1, len(dist_rankings) + 1))\n",
    "\n",
    "aif360_rankings = pd.DataFrame(ranks[aif360_metrics].sum(axis=1).sort_values(ascending=False), columns=['RS**'])\n",
    "aif360_rankings['R**'] = list(range(1, len(aif360_rankings) + 1))\n",
    "\n",
    "rankings_all = pd.DataFrame(ranks.sum(axis=1).sort_values(ascending=False), columns=['RS'])\n",
    "rankings_all['R'] = list(range(1, len(rankings_all) + 1))\n",
    "\n",
    "dataset_rankings = dist_rankings.join(aif360_rankings)\n",
    "dataset_rankings = dataset_rankings.join(rankings_all)\n",
    "dataset_rankings = dataset_rankings.sort_values(by='R')\n",
    "\n",
    "print('Legend:')\n",
    "print('------------------------------')\n",
    "print('RS* = Total Ranking Score for ranking scores which include only \\'distribution\\' metrics')\n",
    "print('      (e.g., entropy, IR & ID)')\n",
    "print('R* = Ranking based on total ranking scores coming only from \\'distribution\\' metrics')\n",
    "print('      (e.g., entropy, IR & ID)')\n",
    "print()\n",
    "print('RS** = Total Ranking Score for ranking scores which include only \\'AIF360\\' metrics')\n",
    "print('      (e.g., SPD, DIR & SEDF)')\n",
    "print('R** = Ranking based on total ranking scores coming only from \\'AIF360\\' metrics')\n",
    "print('      (e.g., SPD, DIR & SEDF)')\n",
    "print()\n",
    "print('RS = Total Ranking Score for ALL ranking scores')\n",
    "print('R = Ranking based on total ranking scores coming from ALL metrics')\n",
    "\n",
    "dataset_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRANKINGS[dataset_id] = stats.join(dataset_rankings).sort_values(by='R')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98d0b5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Results Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c200d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in DRANKINGS:\n",
    "    print(\"DID: \" + str(DATASETS[e]))\n",
    "    display(DRANKINGS[e])\n",
    "    DRANKINGS[e].to_csv(str(DATASETS[e]) + '_results.csv', sep='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509993f1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Verification Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bld = BinaryLabelDataset(\n",
    "    df=data,\n",
    "    label_names=dataset.target_names,\n",
    "    protected_attribute_names=focus,\n",
    "    favorable_label=DSETTINGS[dataset_id].favorable_label, \n",
    "    unfavorable_label=DSETTINGS[dataset_id].unfavorable_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaac737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original_train, dataset_original_test = bld.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc19906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(dataset_original_train.features, dataset_original_train.labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(dataset_original_test.features)\n",
    "\n",
    "classified_dataset = dataset_original_test.copy()\n",
    "classified_dataset.labels = y_pred\n",
    "\n",
    "for attribute in focus:\n",
    "    majority_classes = fr.get_majority_classes(data[attribute])\n",
    "    minority_classes = fr.get_minority_classes(data[attribute])\n",
    "    \n",
    "    privileged_groups = [{attribute: v} for v in majority_classes]\n",
    "    unprivileged_groups = [{attribute: v} for v in minority_classes]\n",
    "    \n",
    "    metric = ClassificationMetric(\n",
    "        dataset_original_test, \n",
    "        classified_dataset, \n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Attribute in focus: \" + attribute)\n",
    "    print(\"False negative rate difference: \" + str(round(metric.false_negative_rate_difference(), 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640267e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brz-biases",
   "language": "python",
   "name": "brz-biases"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
